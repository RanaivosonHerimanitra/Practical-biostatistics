--- 
title: "Practical Biostatistics"
author: "Herimanitra RANAIVOSON"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
#bibliography: [book.bib, packages.bib]
#biblio-style: apalike
link-citations: yes
github-repo: RanaivosonHerimanitra/Practical-biostatistics
description: "A curated list of resources for statisticians, social scientists, biologists, students involved in data analysis and interpretation tasks such as inferential statistics, Bioinformatics. All with straightforward application using R and/or Python programming languages."
---

# About this book:

A curated list of resources for statisticians, social scientists, biologists, students involved in data analysis and interpretation tasks such as inferential statistics, Bioinformatics. All with straightforward application using R and/or Python programming languages.

# Sample distribution and confidence interval

Statisticians never start out knowing the average in the population. They use sampling 
distribution to infer **parameters** of the population. 

## The jargon:

Within a sample, we can calculate a statistic and get its _standard deviation_ which is also called _standard error_ . _standard deviation_ reflects the **spread** of the distribution of the statistics we are interested in.

We can do more with our sample by guessing its confident interval. A numerical interval from within which **true** parameters should fluctuate by calculating what we call _standard error_ by the following formula:

$$ SE=\sigma/\sqrt(n) $$

In case of proportion (our statistics of interest), _standard error_ can be written as follows:

$$ SE=\sqrt( p(1-p) / n )  $$

where standard deviation of proportion is:

$$ \sigma = \sqrt(p(1-p)) $$

## Law of large number and the Central Limit Theorem 

# Type of uncertainty and statistical power:

Statistical power is the dark side of sample size calculation. Survey statisticians often get confused because they used to apply classical sampling theories drawn from survey design methodologies (strata allocation, multistage sampling,etc.) whereas Biostatisticians are more likely to encounter the former when designing their randomized controlled trial studies. As a Biostatistician, you need to master both as health studies may be observational as well as experimental like RCT. So what is statistical power ?

When designing an experiment, one needs to define hypotheses testing:

* A null hypothesis ( either an hypothesis that is more likely to appear either an hypothesis we want to test against ) and

* An alternative hypothesis

Once, they are defined, you can predict the range of values for your statistics of interest that you expect to see if the null hypothesis is true. After data collection, you calculate your statistics and its <code>p-value</code>. If <code>p-value</code> is small enough (let's say less than 5%), you encounter a statistics that is not expect (because It is less than 5%) based on null hypothesis an vice versa.
The statistical power is the probability of finding true effect when It actually exists. It directly relates to incertainty during our decision to reject or null the null hypothesis.

**Summary:**

The probability to reject the _null hypothesis_ when an _alternative hypothesis_ is true, under a certain study conditions, is then called power of a statistical test. Suppose we are a given statistic on mean weight (55 kg) of population in a given village. We are interested in a nutritional survey that concerns people with a mean weight of 50 with a standard deviation of 5kg. We randomly sampled 35 people and then asked to find true population mean of 50 at the 5% level of significance. The probability associated with this level of confidence is called **_statistical power_**.

# Usual Statistical tests:

## t-test and Wilcoxon rank-sum test:

Here, we present the most widely used statistical test called t-test and its counterpart in the non parametric world.


# Non parametric versus parametric test approach:

Non parametric tests approach construct statistic test based on the sample distribution and don't rely on any parameters that describe the population as opposed to parametric test which state that population distribution can be summarized by numerical parameters. One such example is the **Kruskall Wallis test** which is an alternative to ANOVA when sample size is small but also when normality assumption is violated.

## Different types of variables association:

Depending on types of variables, we may encounter 03 cases:
* When variables are quantitative variables, we use coefficient of correlation to measure their association. 03 outcomes are possible: negatively correlated(-1), positively correlated (+1) and no correlation (0).
* When variables are categorical, we use chi square distribution to measure their distance compared to an independence situation.

In case we have mixture of both types, we use variance analysis mainly using correlation

## Testing independence between two categorical variables:
This kind of test is often used in statistical genetics to measure linkage disequilibrium (LD) for gene mapping.

## Testing difference between two sample mean:

## Testing difference between 02 proportions:

**Context**
We would like to draw conclusion on 02 population proportions $$ D=p{1} - p{2} $$ . For example, we may want to know if proportion of Malaria occurrence in Rural area is significantly different from urban one. All we need to do is to build a confidence interval for $$ D $$ then make our conclusion. Our confidence interval relies on normal distribution of the difference between the 02 proportions under certain conditions which must be fulfilled:

(1) $$ n{1}p{1}>10 $$ 

(2) $$ n{1}(1-p{1})>10 $$ 

(3) $$ n{2}p{2}>10 $$ 

(4) $$ n{2}(1-p{2})>10 $$

# Study design in traditional survey:

# Study design in Public Health:

## Sample size in a clinical trial:

Sample size determination is key in trial design. In fact, a well designed trial is a trial that has required sample size to detect clinically meaningful difference between treatment groups.

In most randomized trial, the goal is to demonstrate the _superiority_ of a new regimen (new treatment) with respect to standard or existing one.

Clinical trial can be divided in 02 groups depending on their goal:

* Superiority trial which attempts to demonstrate the _superiority_ of a new treatment as previously stated.

* Non inferiority trial which attempts to demonstrate the _equivalence_ of let's say 02 concurrent treatments.

# Application in development economics:

# Statistical modelling in Epidemiology:

## Linear regression and how to interpret results:

### multiple linear regression:

Is used to jointly assess effects of multiple covariates on an outcome.???

## logistic regression and odds-ratio :

**Roadmap**

* Crude odds-ratio
* Don't forget to mention formula of Mantel-Haenszel 
* Confidence interval for odds-ratio
* Propensity score matching: can be useful when controlling for multiple confounders because it combines candidate confounders into a single summary score. Distinguish cases depending on type of  [https://courses.edx.org/courses/HarvardX/PH207x/2012_Fall/courseware/Week_10/week10:epi10/](studies).

* Classification using logistic regression (Sensitivity: is the capacity of a statistical test to detect true when It's observed to be true (in reality).)

### Instrumental variables and confounders:

## Cox proportional hazard regression:

When we study a cohort of patients

## Machine Learning in public health:


```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```
